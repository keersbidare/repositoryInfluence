# -*- coding: utf-8 -*-
"""cookie_monster_with_tmf_v1.1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15pYabMgs0sxz_NGucLEAK_7ygYgBwt0Z

# Initial shit
"""

import pandas as pd
import csv
import matplotlib.pyplot as plt
from ast import literal_eval

def read_text(file_name):
	data = pd.read_csv(file_name,delimiter=',', encoding="utf-8-sig")
	return data

bin_range = []

# read repo data
repo_df = read_text("repo_data.csv")
repo_df.shape

repo_df.columns

repos_list = repo_df['repo']
authors_list = repo_df['author']
created_date_list = repo_df['created_date']
last_updated_date_list = repo_df['last_updated_date']

from datetime import datetime
def get_datetime(date):
    year = int(date[:4])
    month = int(date[5:7])
    day = int(date[8:10])
    hour = int(date[11:13])
    mint = int(date[14:16])
    secd = int(date[17:])

    return datetime(year, month, day, hour, mint, secd)

get_datetime(last_updated_date_list[0])

repo_created_date_map = {}
for i in range(len(repos_list)):
    date = get_datetime(created_date_list[i])
    repo_created_date_map[repos_list[i]] = date
repo_update_date_map = {}
for i in range(len(repos_list)):
    date = get_datetime(last_updated_date_list[i])
    repo_update_date_map[repos_list[i]] = date
repo_life_map = {}
for repo in repos_list:
    repo_life_map[repo] = repo_update_date_map[repo] - repo_created_date_map[repo]
# repo_life_map

#What is a parent repository?
parent_list = repo_df['parent']
parent_list.isnull().sum()

import csv
import json
import re
import ast

repo_data_fork_time = {}

with open('forks_time.csv', 'r') as csv_file:
    csv_reader = csv.reader(csv_file)
    next(csv_reader)
    for row in csv_reader:
      author_repo = row[0]


      fork_authors = ast.literal_eval(row[1])
      forks = row[2]
      if forks == '[]':
        continue
      else:
        datetime_strings = re.findall(r'datetime\.datetime\(([^)]+)\)', row[2])
        datetimes = []
        for dt_str in datetime_strings:
            dt_params = [int(x) for x in dt_str.split(', ') if not x.startswith('tzinfo')]
            if len(dt_params) == 5:
                dt_params.append(0)
            dt_obj = datetime(*dt_params)
            datetimes.append(dt_obj)

        formatted_datetimes = [str(dt_obj) for dt_obj in datetimes]


      repo_data_fork_time[author_repo] = { }
      for i in range(len(fork_authors)):
        repo_data_fork_time[author_repo][fork_authors[i]] = formatted_datetimes[i]

import csv
import ast
def check_commits(csv_file):
    commits_map = {}
    with open(csv_file, newline='') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            repo_name = row[0]
            commit_timestamps_str = len(row[1].strip())
            if commit_timestamps_str == 2:
              commits_map[repo_name] = False
            else:
              commits_map[repo_name] = True

    return commits_map

repo_commits_map = {}
csv_file = "commits_cp.csv"
repo_commits_map = check_commits(csv_file)

parent_list = repo_df['parent']
parent_list.isnull().sum()

contributors_list = repo_df['contributors_list'].apply(literal_eval)

watchers_list = repo_df['watchers_list'].apply(literal_eval)

forkers_list = repo_df['forkers_list'].apply(literal_eval)

stargazers_list = repo_df['stargazers_list'].apply(literal_eval)

assigness_list = repo_df['assignees_list'].apply(literal_eval)

pull_commenters_list = repo_df['pull_commenter'].apply(literal_eval)

issuers_list = repo_df['issuers_list'].apply(literal_eval)

"""# basic statistics"""

def get_count_map(alist_of_list):
    count_map = {}
    for alist in alist_of_list:
        if len(alist) in count_map:
            count_map[len(alist)] += 1
        else:
            count_map[len(alist)] = 1
    count_map = {k: v for k, v in sorted(count_map.items(), key=lambda item: item[0])}
    return count_map

contributors_count_map = get_count_map(contributors_list)
watchers_count_map = get_count_map(watchers_list)
forkers_count_map = get_count_map(forkers_list)
stargazers_count_map = get_count_map(stargazers_list)
assigness_count_map = get_count_map(assigness_list)
pull_commenters_count_map = get_count_map(pull_commenters_list)
issuers_count_map = get_count_map(issuers_list)

"""# remove benign things from the dataset"""

def remove_benign_authors(alist_of_list, malware_authors):
    new_list = []
    for alist in alist_of_list:
        temp = []
        for val in alist:
            if val in malware_authors:
                temp.append(val)
        new_list.append(temp)
    return new_list

contributors_list2 = remove_benign_authors(contributors_list, list(authors_list))
watchers_list2 = remove_benign_authors(watchers_list, list(authors_list))
forkers_list2 = remove_benign_authors(forkers_list, list(authors_list))
stargazers_list2 = remove_benign_authors(stargazers_list, list(authors_list))
assigness_list2 = remove_benign_authors(assigness_list, list(authors_list))
pull_commenters_list2 = remove_benign_authors(pull_commenters_list, list(authors_list))
issuers_list2 = remove_benign_authors(issuers_list, list(authors_list))

contributors_count_map2 = get_count_map(contributors_list2)
watchers_count_map2 = get_count_map(watchers_list2)
forkers_count_map2 = get_count_map(forkers_list2)
stargazers_count_map2 = get_count_map(stargazers_list2)
assigness_count_map2 = get_count_map(assigness_list2)
pull_commenters_count_map2 = get_count_map(pull_commenters_list2)
issuers_count_map2 = get_count_map(issuers_list2)

def create_repo_author_map(alist_of_list, repos_list):
    repo_author_map = {}
    for i in range(0, len(repos_list)):
        if len(alist_of_list[i]) > 0:
                repo_author_map[repos_list[i]] = alist_of_list[i]
    return repo_author_map

contributors_map = create_repo_author_map(contributors_list2, repos_list)
watchers_map = create_repo_author_map(watchers_list2, repos_list)
forkers_map = create_repo_author_map(forkers_list2, repos_list)
stargazers_map = create_repo_author_map(stargazers_list2, repos_list)
assigness_map = create_repo_author_map(assigness_list2, repos_list)
pull_commenters_map = create_repo_author_map(pull_commenters_list2, repos_list)
issuers_map = create_repo_author_map(issuers_list2, repos_list)

repo_authors_map = {}
for i in range(0, len(repos_list)):
    repo_authors_map[repos_list[i]] = authors_list[i]
author_parent_map = {}
repo_parent_map = {}
parent_list = list(parent_list)
#count = 0
for i in range(0, len(authors_list)):
    if type(parent_list[i]) != float:
        author_parent_map[authors_list[i]] = parent_list[i]
        repo_parent_map[repos_list[i]] = parent_list[i]
    else:
        author_parent_map[authors_list[i]] = authors_list[i]
        repo_parent_map[repos_list[i]] = repos_list[i]

len(repo_authors_map)



"""# user data"""

user_df = read_text('user_data.csv')
user_df.shape

author_from_udata = user_df['author']
created_date_author = user_df['created_at']
update_date_author = user_df['updated_at']
repo_count_list = user_df['repo_count']

author_created_date_map = {}
for i in range(len(author_from_udata)):
    date = get_datetime(created_date_author[i])
    author_created_date_map[author_from_udata[i]] = date
author_update_date_map = {}
for i in range(len(author_from_udata)):
    date = get_datetime(update_date_author[i])
    author_update_date_map[author_from_udata[i]] = date
author_life_map = {}
for author in author_from_udata:
    author_life_map[author] = author_update_date_map[author] - author_created_date_map[author]
# author_life_map

followers_list = user_df['followers_list'].apply(literal_eval)
followers_list2 = remove_benign_authors(followers_list, list(authors_list))

author_follower_map = {}
for i in range(0, len(author_from_udata)):
    author_follower_map[author_from_udata[i]] = followers_list2[i]

"""# create author-author network"""

import networkx as nx
aa = nx.DiGraph()

# def get_avg_degree(G):
#     degrees = G.degree()
#     degrees = dict(degrees)
#     sum_of_edges = sum(degrees.values())
#     no_nodes = G.number_of_nodes()
#     avg_degree = sum_of_edges/1616
#     return avg_degree

"""OUR CHANGES"""

#  D_Follower
follower_edge_count = 0
for author in author_from_udata:
  follower_edge_count += len(author_follower_map[author])

# D_Fork
fork_edge_count = 0
for repo, forkers in forkers_map.items():
    fork_edge_count += len(forkers)

# D_Star
star_edge_count = 0
for repo, stargazers in stargazers_map.items():
    star_edge_count += len(stargazers)

# D_Watch
watch_edge_count = 0
for repo, watchers in watchers_map.items():
    watch_edge_count += len(watchers)

# D_Comment
comment_edge_count = 0
for repo, commenters in issuers_map.items():
    comment_edge_count += len(commenters)

# D_Contributer
contributer_edge_count = 0
for repo, contributers in contributors_map.items():
    contributer_edge_count += len(contributers)

# contributors_map

follower_edge_count, fork_edge_count, star_edge_count, watch_edge_count, comment_edge_count, contributer_edge_count

d_min = min(follower_edge_count, fork_edge_count, star_edge_count, watch_edge_count,contributer_edge_count, comment_edge_count)
d_min

d_follower = follower_edge_count / len(repo_authors_map)
d_fork = fork_edge_count / len(repo_authors_map)
d_star = star_edge_count / len(repo_authors_map)
d_watch = watch_edge_count / len(repo_authors_map)
d_contributor = contributer_edge_count / len(repo_authors_map)
d_commenter = comment_edge_count / len(repo_authors_map)
d_min = min(d_follower, d_fork, d_star, d_watch, d_commenter)

# w_follower = (d_min/d_follower)*10
# w_fork = (d_min/d_fork)*10
# w_star = (d_min/d_star)*10
# w_watch = (d_min/d_watch)*10
# w_contributor = (d_min/d_contributor)*10
# w_commenter = d_min / d_commenter

w_follower = (d_min/d_follower)
w_fork = (d_min/d_fork)
w_star = (d_min/d_star)
w_watch = (d_min/d_watch)
w_contributor = (d_min/d_contributor)
w_commenter = d_min / d_commenter

# w_follower = 0.3
# w_fork = 0.15
# w_star = 0.5
# w_watch = 0.27
# w_contributor = d_min/d_contributor
# w_commenter = 1

print(w_follower, w_fork, w_star, w_watch,w_contributor,  w_commenter)



"""OUR CHANGES END"""

for author in author_from_udata:
    if len(author_follower_map[author]) != 0:
        for follower in author_follower_map[author]:
            aa.add_weighted_edges_from([(follower, author, w_follower)])

aa

followers_nodes = len(list(aa.nodes))
followers_edges = len(list(aa.edges))

# add edge from forking relationship
update_count = 0
new_edge_fork = 0
for repo, forkers in forkers_map.items():
    original_author = repo_authors_map[repo]
    #print("orginal :", original_author)
    for user in forkers:
        #print( aa.number_of_edges(user, original_author))
        if aa.has_edge(user, original_author):
            #print("Already edges there")
            old_weight = aa.get_edge_data(user, original_author)['weight']
            #print(old_weight)
            aa.add_weighted_edges_from([(user, original_author, old_weight+w_fork)])
            update_count += 1
            # print("Edge already exists: ", user, original_author)
        else:
            aa.add_weighted_edges_from([(user, original_author, w_fork)])
            new_edge_fork += 1

total_nodes = len(list(aa.nodes))
total_edges = len(list(aa.edges))
#new_edges = total_edges - update_count
new_nodes = total_nodes - followers_nodes
print("Total nodes after adding forking edge: ", total_nodes)
print("Total edges after adding forking edge: ", total_edges)
print("Number of edges that have been updated: ", update_count)
print("Number of new edges that have been added: ", new_edge_fork)
print("Number of new nodes that have been added: ", new_nodes)

# add edges from stars behavior
update_in_stars = 0
new_edge = 0
starrer = 0
for repo, stargazers in stargazers_map.items():
    original_author = repo_authors_map[repo]
    for user in stargazers:
        starrer += 1
        if aa.has_edge(user, original_author):
            old_weight = aa.get_edge_data(user, original_author)['weight']
            aa.add_weighted_edges_from([(user, original_author, old_weight+w_star)])
            update_in_stars += 1
        else:
            aa.add_weighted_edges_from([(user, original_author, w_star)])
            new_edge += 1

total_nodes_adding_stars = len(list(aa.nodes))
total_edges_adding_stars = len(list(aa.edges))

print("Total nodes after adding starring edge: ",total_nodes_adding_stars)
print("Total edges after adding starring edge: ", total_edges_adding_stars)
print("Number of edges that have been updated: ", update_in_stars)
print("Number of new edges that have been added: ", new_edge)
print("total starrer: ", starrer)

# add edges from watches behavior
update_in_watches = 0
new_edge = 0
watcher = 0
for repo, watchers in watchers_map.items():
    original_author = repo_authors_map[repo]
    for user in watchers:
        watcher += 1
        if aa.has_edge(user, original_author):
            old_weight = aa.get_edge_data(user, original_author)['weight']
            aa.add_weighted_edges_from([(user, original_author, old_weight+w_watch)])
            update_in_watches += 1
        else:
            aa.add_weighted_edges_from([(user, original_author, w_watch)])
            new_edge += 1

total_nodes_adding_watches = len(list(aa.nodes))
total_edges_adding_watches = len(list(aa.edges))
print("Total nodes after adding watching edge: ",total_nodes_adding_watches)
print("Total edges after adding watching edge: ", total_edges_adding_watches)
print("Number of edges that have been updated: ", update_in_watches)
print("Number of new edges that have been added: ", new_edge)
print("total watcher: ", watcher)

# add edges from contributors behavior
update_in_contributors = 0
new_edge = 0
contributor = 0
for repo, contributors in contributors_map.items():
    original_author = repo_authors_map[repo]
    for user in contributors:
        contributor += 1
        if aa.has_edge(user, original_author):
            old_weight = aa.get_edge_data(user, original_author)['weight']
            aa.add_weighted_edges_from([(user, original_author, old_weight+w_contributor)])
            update_in_contributors += 1
        else:
            aa.add_weighted_edges_from([(user, original_author, w_contributor)])
            new_edge += 1

# add edges from commenting behavior
update_in_commenters = 0
new_edge = 0
commenter = 0
for repo, issuers in issuers_map.items():
    original_author = repo_authors_map[repo]
    for user in issuers:
        commenter += 1
        if aa.has_edge(user, original_author):
            old_weight = aa.get_edge_data(user, original_author)['weight']
            aa.add_weighted_edges_from([(user, original_author, old_weight+w_commenter)])
            update_in_commenters += 1
        else:
            aa.add_weighted_edges_from([(user, original_author, w_commenter)])
            new_edge += 1

"""# compute popular/famous/influential authors"""

from sklearn import preprocessing as prep

def normalized_score(score_map):
    authors = []
    values = []
    for key, val in score_map.items():
        authors.append(key)
        values.append(val)
    values = prep.normalize([values])
    #print(len(values))
    new_map = {}
    for i in range(len(authors)):
        new_map[authors[i]] = values[0][i]
    return new_map

hubs, authorities = nx.hits(aa, max_iter = 522, normalized = True)
sorted_auth = {k: v for k, v in sorted(authorities.items(), key=lambda item: item[1])}
sorted_hubs = {k: v for k, v in sorted(hubs.items(), key=lambda item: item[1])}
auths = normalized_score(sorted_auth)
hubs = normalized_score(sorted_hubs)

# hubs, auths

"""# compute RepoPop"""

# compute repository popularity using stars-forks-watch
watchers_counts = []
stargazers_counts = []
forkers_counts = []
for i in range(len(repos_list)):
    watchers_counts.append(len(watchers_list2[i]))
    stargazers_counts.append(len(stargazers_list2[i]))
    forkers_counts.append(len(forkers_list2[i]))

import numpy as np
from sklearn import preprocessing as prep

watchers_counts = np.array(watchers_counts)
watchers_counts = prep.normalize([watchers_counts])
watchers_counts = watchers_counts.tolist()


forkers_counts = np.array(forkers_counts)
forkers_counts = prep.normalize([forkers_counts])
forkers_counts = forkers_counts.tolist()


stargazers_counts = np.array(stargazers_counts)
stargazers_counts = prep.normalize([stargazers_counts])
stargazers_counts = stargazers_counts.tolist()

stargazers_counts = stargazers_counts[0]
watchers_counts = watchers_counts[0]
forkers_counts = forkers_counts[0]

repo_popularity_map = {}

for i in range(len(repos_list)):
    score = watchers_counts[i]/3 + forkers_counts[i]/3 + stargazers_counts[i]/3
    repo_popularity_map[repos_list[i]] = score
#repo_popularity_map

"""
















# Compute plausibility score"""

authors_repo_map = {}
for repo,author in repo_authors_map.items():
    if author in authors_repo_map:
        authors_repo_map[author].append(repo)
    else:
        authors_repo_map[author] = [repo]
#authors_repo_map

total_stargazers = 0
total_forkers = 0
total_watchers = 0
total_contributors = 0
total_issuers = 0
for repo in repos_list:
    if repo in stargazers_map:
        total_stargazers += len(stargazers_map[repo])
    if repo in forkers_map:
        total_forkers += len(forkers_map[repo])
    if repo in watchers_map:
        total_watchers += len(watchers_map[repo])
    if repo in contributors_map:
        total_contributors += len(contributors_map[repo])
    if repo in issuers_map:
        total_issuers += len(issuers_map[repo])

total_stargazers /= len(repos_list)
total_forkers /= len(repos_list)
total_watchers /= len(repos_list)
total_issuers += total_contributors
total_issuers /= len(repos_list)
total_stargazers, total_forkers, total_watchers, total_issuers

wc = 1
ws = total_issuers/total_stargazers
wf = total_issuers/total_forkers
ww = total_issuers/total_watchers
ws, wf, ww, wc

total_follower = 0
for key, val in author_follower_map.items():
    total_follower += len(val)
print(total_follower)
total_follower/len(author_follower_map)

def get_other_repo_contributions(stargazers_map):
    number_of_authors = len(set(authors_list))
    count = 0
    temp_map = {}
    for repo, stars in stargazers_map.items():
        repo_author = repo[:repo.find('/')]
        if repo_author in temp_map:
            temp_map[repo_author].extend(stars)
        else:
            temp_map[repo_author] = stars

    for key, val in temp_map.items():
        list_count = len(val)
        set_count = len(set(val))
        count += list_count/set_count
    other_repo_contributors = count/number_of_authors

    return other_repo_contributors

other_repo_contributors = get_other_repo_contributions(stargazers_map) + get_other_repo_contributions(watchers_map) + get_other_repo_contributions(forkers_map)
other_repo_contributors

w_fol = 1
w_orc = 0.46/other_repo_contributors
w_orc

ra_arr = []
aa_arr = []
hub_arr = []
auth_arr = []
pi_arr = []



"""TMF"""

from collections import Counter
import numpy as np
from scipy import stats
#Find the maximum time difference in years
time = []
maximum_difference_years = None
time_difference_years = None
minimum_difference_years = None
for repo1 in repos_list:
        for repo2 in repos_list:
            if repo_authors_map[repo1] != repo_authors_map[repo2]:
              date1 = repo_created_date_map[repo1]
              date2 = repo_created_date_map[repo2]
              time_difference_seconds = abs((date1 - date2).total_seconds())
              time_difference_years = time_difference_seconds / (365.25 * 24 * 3600)
              time.append(time_difference_years)

            if maximum_difference_years is None or time_difference_years > maximum_difference_years:
                maximum_difference_years = time_difference_years
            if minimum_difference_years is None or time_difference_years < minimum_difference_years:
                if time_difference_years != 0.0:
                  minimum_difference_years = time_difference_years

time_differences_array = np.array(time)
mean = np.mean(time_differences_array)
median = np.median(time_differences_array)
time_differences_array = [value for value in time_differences_array if value != 0]
frequency_count = Counter(time_differences_array)
max_frequency = max(frequency_count.values())
mode = [key for key, value in frequency_count.items() if value == max_frequency]
sorted_frequency_count = dict(sorted(frequency_count.items(), key=lambda item: item[1], reverse=True))




maximum_difference_years,minimum_difference_years,mean,median

file_name = "sorted_frequency_count_for_repos.txt"
with open(file_name, "w") as file:
    for key, value in sorted_frequency_count.items():
        file.write(f"{key}: {value}\n")

all_time_diff_lists =[]

import csv
import json
import re

repo_data_fork_time = {}

with open('forks_time.csv', 'r') as csv_file:
    csv_reader = csv.reader(csv_file)
    next(csv_reader)
    for row in csv_reader:
      author_repo = row[0]
      forks = row[2]
      if forks == '[]':
        continue
      else:

        # datetime_strings = re.findall(r'datetime\.datetime\(([^)]+)\)', row[2])
        # datetimes = []
        # for dt_str in datetime_strings:
        #     dt_params = [int(x) for x in dt_str.split(', ') if not x.startswith('tzinfo')]
        #     if len(dt_params) == 5:
        #         dt_params.append(0)
        #     dt_obj = datetime(*dt_params)
        #     datetimes.append(dt_obj)

        # formatted_datetimes = [str(dt_obj) for dt_obj in datetimes]

        datetime_info = row[2]
        pattern = r"datetime\.datetime\((\d+), (\d+), (\d+), (\d+), (\d+), (\d+),"
        matches = re.findall(pattern, datetime_info)
        formatted_datetimes = []
        match = re.search(pattern, datetime_info)
        for match in matches:
          year, month, day, hour, minute, second = map(int, match)
          dt = datetime(year, month, day, hour, minute, second)
          formatted_datetimes.append(dt)
      repo_data_fork_time[author_repo] = formatted_datetimes
list_timediff_fork = []
for key,value in repo_data_fork_time.items():
  date = repo_created_date_map[key]
  for i in value:
    # print(type(datetime(i)), type(date))
    time_difference_seconds = abs((date - i).total_seconds())
    time_difference_years = time_difference_seconds / (365.25 * 24 * 3600)
    list_timediff_fork.append(time_difference_years)

all_time_diff_lists.append(list_timediff_fork)

import csv
import json
import re

repo_data_pullrequests_time = {}

with open('pr_time_test.csv', 'r') as csv_file:
    csv_reader = csv.reader(csv_file)
    next(csv_reader)
    for row in csv_reader:
      author_repo = row[0]
      pr_requests = row[2]
      if pr_requests == '[]':
        continue
      else:
        datetime_info = row[2]
        pattern = r"datetime\.datetime\((\d+), (\d+), (\d+), (\d+), (\d+), (\d+),"
        matches = re.findall(pattern, datetime_info)
        formatted_datetimes = []
        match = re.search(pattern, datetime_info)
        for match in matches:
          year, month, day, hour, minute, second = map(int, match)
          dt = datetime(year, month, day, hour, minute, second)
          formatted_datetimes.append(dt)
      repo_data_pullrequests_time[author_repo] = formatted_datetimes
list_timediff_pullrequests = []
for key,value in repo_data_pullrequests_time.items():
  date = repo_created_date_map[key]
  for i in value:
    time_difference_seconds = abs((date - i).total_seconds())
    time_difference_years = time_difference_seconds / (365.25 * 24 * 3600)
    list_timediff_pullrequests.append(time_difference_years)

all_time_diff_lists.append(list_timediff_pullrequests)

import openpyxl
from collections import Counter
workbook = openpyxl.load_workbook('repo_data_stargazers (1).xlsx')
sheet = workbook.active
repo_names_for_starred = []
repo_starred_time = []
for row in sheet.iter_rows(min_row=2, values_only=True):
    repo_name = row[0]
    starred_time = row[3]
    repo_names_for_starred.append(repo_name)
    # print(starred_time)
    repo_starred_time.append(get_datetime(starred_time))
minimum_difference_years = None
list_timediff_stars = []
for i in range(len(repo_names_for_starred)):
    date = repo_created_date_map[repo_names_for_starred[i]]
    time_difference_seconds = abs((date - repo_starred_time[i]).total_seconds())
    time_difference_years = time_difference_seconds / (365.25 * 24 * 3600)
    list_timediff_stars.append(time_difference_years)

hashmap_stars = Counter(list_timediff_stars)
all_time_diff_lists.append(list_timediff_stars)

num_bins = 22
max_years = 11
bin_width = max_years / num_bins
bins = [bin_width * (i + 1) for i in range(num_bins)]
bin_values = {bin: 0 for bin in bins}

i = 1
for List in all_time_diff_lists:
  for value in List:
    for bin in bins:
      if bin <= value <= (bin+0.5):
        bin_values[bin] += 1
        break

bin_values

occurrences = []
for bin, value in bin_values.items():
    occurrences.append(value)

plt.figure(figsize=(10, 6))
plt.bar(bins, occurrences, width=0.4, color='skyblue', align='center')
plt.xlabel('Time (years)')
plt.ylabel('Occurrences')
plt.title('Occurrences vs Time')
plt.xticks(bins, [f'{bin:.1f}-{bin+0.5:.1f}' for bin in bins], rotation=45)
plt.bar_label(plt.gca().containers[0])
plt.tight_layout()
plt.show()

def min_max_normalize(data):
    min_val = min(data)
    max_val = max(data)
    normalized_data = [(x - min_val) / (max_val - min_val) for x in data]
    return normalized_data

normalized_values = min_max_normalize(occurrences)
time_normalized_map = {}
for key, value in zip(bins,normalized_values):
    time_normalized_map[key] = value

def collect_temporal_data(time):
  time = abs(time)
  tmf_value = 0
  time_in_seconds = time.total_seconds()
  time_in_years = time_in_seconds / (365.25 * 24 * 3600)
  for bin in time_normalized_map:
        if bin <= time_in_years <= (bin+0.5):
            tmf_value = time_normalized_map[bin]
            break
  return tmf_value

"""Compute Plausibilty"""

def compute_plausibility1(repo1, repo2):
    author1 = repo_authors_map[repo1]
    author2 = repo_authors_map[repo2]
    date1 = repo_created_date_map[repo1]
    date2 = repo_created_date_map[repo2]
    time = date1 - date2

    tmf_value = 1
    if time.days > 0:
      tmf_value = collect_temporal_data(time)
      # return { "repo1" : repo1, "repo2": repo2, "plausibility" : 0, "ra_score" : 0, "aa_score" : 0, "hub_u" : 0, "auths_u": 0, "notes" : "new_to_old"}

    #  RA Score Components

    fork_score = 0
    if repo1 in forkers_map:
        if author2 in forkers_map[repo1]:

          if author1 in repo_data_fork_time and repo_data_fork_time[author1][author2] - date2 == 0: # is R2 the repo forked from R1
            if repo_commits_map[repo2]:
              fork_score = 1
            else:
              fork_score = 0.5
          else:
            fork_score = 0.25  # not very directly influenced

    star_score = 0
    if repo1 in stargazers_map:
        if author2 in stargazers_map[repo1]:
            star_score = 1


    watch_score = 0
    if repo1 in watchers_map:
        if author2 in watchers_map[repo1]:
            watch_score = 1

    comment_score = 0
    if repo1 in issuers_map:
        if author2 in issuers_map[repo1]:
            comment_score = 1

    contribution_score = 0
    if repo1 in contributors_map:
        if author2 in contributors_map[repo1]:
            contribution_score = 1


    #  AA Score Components

    follow_score = 0
    if author1 in author_follower_map:
        if author2 in author_follower_map[author1]:
            follow_score = 1



    other_repo_star_score = 0
    if author1 in authors_repo_map:
        if len(authors_repo_map[author1]) > 1:
            for repo in authors_repo_map[author1]:
                if repo in stargazers_map:
                    if author2 in stargazers_map[repo]:
                        other_repo_star_score += 1

    other_repo_fork_score = 0
    if author1 in authors_repo_map:
        if len(authors_repo_map[author1]) > 1:
            for repo in authors_repo_map[author1]:
                if repo in forkers_map:
                    if author2 in forkers_map[repo]:
                        other_repo_fork_score += 1





    other_repo_watch_score = 0
    if author1 in authors_repo_map:
        if len(authors_repo_map[author1]) > 1:
            for repo in authors_repo_map[author1]:
                if repo in watchers_map:
                    if author2 in watchers_map[repo]:
                        other_repo_watch_score += 1

    other_repo_comment_score = 0
    if author1 in authors_repo_map:
        if len(authors_repo_map[author1]) > 1:
            for repo in authors_repo_map[author1]:
                if repo in issuers_map:
                    if author2 in issuers_map[repo]:
                        other_repo_comment_score += 1

    other_repo_contribution_score = 0
    if author1 in authors_repo_map:
        if len(authors_repo_map[author1]) > 1:
            for repo in authors_repo_map[author1]:
                if repo in contributors_map:
                    if author2 in contributors_map[repo]:
                        other_repo_contribution_score += 1

    famous_score = 0
    if author1 in hubs and author2 in hubs:
        if hubs[author1] > hubs[author2]:
            famous_score = 1
    ra_score = ((star_score) + (fork_score) + (watch_score) + ((comment_score))) /  4
    aa_score = (follow_score + other_repo_fork_score + other_repo_star_score + other_repo_comment_score + other_repo_watch_score ) / 5
    hub_u = hubs[author1] if author1 in hubs  else 0
    auths_u = auths[author1] if author1 in auths  else 0


    popularity = hub_u + auths_u
    # print(ra_score, aa_score, hub_u, auths_u)
    ra_arr.append(ra_score)
    aa_arr.append(aa_score)
    hub_arr.append(hub_u)
    auth_arr.append(auths_u)
    # plausibility = tmf_value * (0.65 * ra_score + 0.25 * aa_score + 0.1 * popularity)
    plausibility = tmf_value * (0.65 * ra_score + 0.25 * aa_score + 0.1*popularity)

    # pi_arr.append({ "repo1" : repo1, "repo2": repo2, "score" : plausibility, "ra_score" : ra_score, "aa_score" : aa_score, "hub_u" : hub_u, "auths_u": auths_u})
    return { "repo1" : repo1, "repo2": repo2, "plausibility" : plausibility, "ra_score" : ra_score, "aa_score" : aa_score, "hub_u" : hub_u, "auths_u": auths_u}
    # return plausibility

plausibility_map = {}



repo_influenced_repos = {}
influenced_me_repos = {}
for repo1 in repos_list:
    for repo2 in repos_list:
        if repo_authors_map[repo1] != repo_authors_map[repo2]:
            obj = compute_plausibility1(repo1, repo2)
            plausibility = obj["plausibility"]
            if repo1 in plausibility_map:
              plausibility_map[repo1][repo2] = plausibility
            else:
              plausibility_map[repo1] = { repo2 : plausibility }

def get_influenced_repos(t):
  repo_influenced_repos = {}
  for repo, obj in plausibility_map.items():
    for key, val in obj.items():
      # print(key, val)
      if(val > t):
        if key in influenced_me_repos:
          influenced_me_repos[key].append([repo, val])
        else:
          influenced_me_repos[key] = [[repo, val]]
        if repo in repo_influenced_repos:
          repo_influenced_repos[repo].append([key, val])
        else:
          repo_influenced_repos[repo] = [[key, val]]
  return repo_influenced_repos, influenced_me_repos



repo_influenced_repos, influenced_me_repos = get_influenced_repos(0.2)

only_out = []

# total_edges = 0
# for key,value in repo_influenced_repos.items():
#   total_edges += len(value)
# total_nodes = 0
# li = []
# for key,value in repo_influenced_repos.items():
#   if key not in li:
#     total_nodes += 1
#     li.append(key)
#   for val in value:
#     if val not in li:
#       total_nodes+=1
#       li.append(val)


# total_edges,total_nodes

"""# PIG"""

def create_pig(threshold):
    pig = nx.DiGraph()
    E = 0
    for repo1 in repos_list:
        for repo2 in repos_list:
            if repo_authors_map[repo1] != repo_authors_map[repo2]:
                # obj = compute_plausibility1(repo1, repo2)
                # plausibility = obj["plausibility"]
                plausibility = plausibility_map[repo1][repo2]
                if plausibility >= threshold:
                  pi_arr.append(obj)
                  if pig.has_edge(repo1, repo2):
                    continue
                  else:
                    E += 1
                    pig.add_weighted_edges_from([(repo1, repo2, plausibility)])
    #print(E)
    return pig

pi_sorted = sorted(pi_arr, key=lambda x: x['plausibility'], reverse=True)

# import csv

# field_names = pi_sorted[0].keys()


# csv_file = 'pi_sorted.csv'

# # Writing to CSV file
# with open(csv_file, 'w', newline='') as f:
#     writer = csv.DictWriter(f, fieldnames=field_names)

#     # Write header
#     writer.writeheader()

#     # Write rows
#     writer.writerows(pi_sorted)


# new_pi_map = {}
# for pi_obj in pi_sorted:
#   if pi_obj["repo1"] in new_pi_map:
#     new_pi_map[pi_obj["repo1"]].append(pi_obj["repo2"])
#   else:
#     new_pi_map[pi_obj["repo1"]] = [pi_obj["repo2"]]



# len(new_pi_map.keys())

# # Specify the name of the CSV file
# op_file = 'pi_map.csv'

# # Writing to CSV file
# with open(op_file, 'w', newline='') as f:
#     writer = csv.writer(f)

#     # Write header
#     writer.writerow(['Repo', 'Influenced Repos'])

#     # Write key-value pairs as rows
#     for key, value in new_pi_map.items():
#         writer.writerow([key, value])

pi_set = set()
for pi_obj in pi_sorted:
  pi_set.add(pi_obj["repo1"])
  pi_set.add(pi_obj["repo2"])

pig_main = create_pig(0.25)
d = pig_main.degree()

nodelist = []
node_size = []
for key, val in pig_main.degree():
    nodelist.append(key)
    node_size.append(val * 10)

nx.draw(pig_main, nodelist=nodelist, node_size=node_size)

pig = create_pig(0.3)
d = pig.degree()

nodelist = []
node_size = []
for key, val in pig.degree():
    nodelist.append(key)
    node_size.append(val * 10)

nx.draw(pig, nodelist=nodelist, node_size=node_size)

t_nodes = list(pig.nodes)
print(len(t_nodes))

# t_list = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
# deg = []
# nodes = []
# edges = []
# for t in t_list:
#     pig = create_pig(t)

#     no_nodes = len(list(pig.nodes))
#     no_edges = len(list(pig.edges))
#     if no_nodes == 0:
#       avg_deg = 0
#     else:
#       avg_deg = no_edges/no_nodes

#     deg.append(avg_deg)
#     nodes.append(no_nodes)
#     edges.append(no_edges)
#     print(t, " : avg degree - ", avg_deg, " no of nodes - ", no_nodes, " no of edges - ", no_edges)

t_list = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
deg = []
nodes = []
edges = []
for t in t_list:
    pig = create_pig(t)

    no_nodes = len(list(pig.nodes))
    no_edges = len(list(pig.edges))
    if no_nodes == 0:
      avg_deg = 0
    else:
      avg_deg = no_edges/no_nodes

    deg.append(avg_deg)
    nodes.append(no_nodes)
    edges.append(no_edges)
    print(t, " : avg degree - ", avg_deg, " no of nodes - ", no_nodes, " no of edges - ", no_edges)

from matplotlib.pyplot import figure
figure(figsize=(10, 6), dpi=80)

nodes[0] = 2034
fig = plt.figure()
plt.plot(t_list, deg, color='green', label = 'avg degree')
plt.plot(t_list, nodes, color='red', label = 'no of nodes')
plt.plot(t_list, edges, color='blue', label = 'no of edges')
plt.legend(loc = 'upper right', fontsize=18)
plt.yscale("log")
plt.xlabel("Plausible Influence Threshold, PIT", fontsize=18)
plt.ylabel("Log", fontsize=16)
plt.show()
fig.savefig('pis_threshold.png')

"""**Dataset**
1. A different group of malware repositories
2.---Somebody should check malware families in our present dataset  1.Removing a few repositories from the dataset, and---?? Can be honest that we did not get the original dataset.-----






*----Draw conclusions on malware repositories behaviour if there are any similar characteristics between all the graphs---*  PIt vs outdegree.

**Inferences made**

*   In the given dataset there are no nodes after 0.9, indicating that no repositories have got extremely influenced or no repositries have influenced anyone strongly.
*   Inspite of this, we see that the  average number of nodes at each level of influence is more than the original PIMAN dataset, meaning this dataset has  repositories have interacted with each other more than the previous data set.
*   Our PIT is 0.3, after observing the graph and the knee, so that we capture the non-trivial influence.
*   Our average degree is very less meaning all the repositories have influenced each other but there are no repositiries with "cult" behaviour -- tree. --?no
*   From the above graph we also observe that the number of edges is always greater than the number of nodes, so there is almost never one-to-one mapping.
"""

l = 0
for key,value in repo_influenced_repos.items():
  #print(f"{key}:{value}")
  l+=len(value)

l

outdegree_repo = dict(sorted(repo_influenced_repos.items(), key=lambda item: len(item[1]),reverse = True))
indegree_repo = dict(sorted(influenced_me_repos.items(), key=lambda item: len(item[1]),reverse = True))

i = 0
outdegree_only = {}
for key,value in outdegree_repo.items():
  outdegree_only[key] = len(value)
  # print(len(value))
  # #print(f"{key}:{value}")
  #print(f"{key}")
  # if i == 1:
  #   break

#outdegree_only

# len(outdegree_repo)

i = 0
indegree_only = {}
for key,value in indegree_repo.items():
  indegree_only[key] = len(value)
  #print(len(value))

#len(repo_author_map[take_auth])

"""**Inferences drawn on in-degree and outdegree**

*   The repository **"androidtrojan1/android-trojan-starter-"** has influenced 195 repositories which is approximately equal to 9.5% of the total datset.This is much higher than the one found in the piman dataset.
*   The indegree of the first 20 repositories is 31.In which the first 19 are from an author '**5l1v3r1**'.After manual observation we realized that all the 19 repositories are infact influenced by the same set of repositories introducing the "cookie monster problem".
"""

avg_score = 0
tpi_map = {}
i = 0
for key,out_val in outdegree_only.items():
  tpi_map[key] = 0
  for value in repo_influenced_repos[key]:
    if len(repo_influenced_repos[key]) != outdegree_only[key]:
      print("NOOO")
      break
    i += 1
    tpi_map[key] += value[1]
  avg_score = (tpi_map[key]/out_val)
avg_score = avg_score / i

avg_score



import matplotlib.pyplot as plt
x = [tpi_map[key] for key in tpi_map]
y = [outdegree_only[key] for key in outdegree_only]
m, b = np.polyfit(x, y, 1)
plt.scatter(x, y)
plt.plot(x, np.multiply(m,x) + b, color='black', label=f'Linear Regression (y = {m:.2f}x + {b:.2f})')
plt.xlabel('TPI Map Values')
plt.ylabel('Outdegree Only Values')
plt.title('Scatter Plot')
plt.legend()
plt.show()

"""**Inferences from the above graph.**
*   Proves that the instensity of influence is directly correlated to the outdegree. It shows the same pattern as PIMAN.
*   There were a few niche repositories in PIMAN where there was high TPI score for less out-degree but that is not the case here, re-proving non-cult behaviour.

**In degree and out-degree heat-map with frequency.**
"""

import seaborn as sns
import matplotlib.pyplot as plt

all_repos = set(indegree_only.keys()).union(outdegree_only.keys())
indegree_values = [indegree_only.get(repo, 0) for repo in all_repos]
outdegree_values = [outdegree_only.get(repo, 0) for repo in all_repos]

indeg_with_repo = [(indegree_only.get(repo, 0), repo) for repo in all_repos]
outdeg_with_repo = [(outdegree_only.get(repo, 0), repo) for repo in all_repos]

sns.histplot(x=outdegree_values, y=indegree_values, bins=20, cbar=True)
plt.xlabel('Out-degree')
plt.ylabel('In-degree')
plt.title('Frequency Heatmap of In-degree vs Out-degree')
plt.show()

"""**Connected components**

We are going to consider weakly connected components because

strongly_connected:-subset of nodes in which every node is reachable from every other node within the subset.subset of nodes in which every node is reachable from every other node within the subset.

weakly connected:-  a weakly connected component of a directed graph is a subset of nodes in which every node is reachable from every other node, but ignoring the direction of the edges.

"""

pig = create_pig(0.3)

connected_components = list(nx.weakly_connected_components(pig))

#print(len(connected_components))
# Print connected components
connected_components = sorted(connected_components,key = len,reverse = True)
for n in connected_components:
  print(len(n))
#dict(sorted(connected_components.items(), key=lambda item: len(item[1]),reverse = True))
# for i, component in enumerate(connected_components, 1):
#   print(f"Connected Component {i}: {component}")

print(connected_components[0])

import matplotlib.pyplot as plt

subgraph = pig.subgraph(connected_components[0])
plt.figure()
nx.draw(subgraph,node_color='blue', node_size=10, font_size=12)
plt.title(f'Connected Component')
plt.show()

t_nodes = list(subgraph.nodes)
print(len(t_nodes))

outdegree_dict = {node: subgraph.out_degree(node) for node in subgraph.nodes}
indegree_dict = {node: subgraph.in_degree(node) for node in subgraph.nodes}
outdegree_repo_sub = dict(sorted(outdegree_dict.items(), key=lambda item: item[1],reverse = True))
indegree_repo_sub = dict(sorted(indegree_dict.items(), key=lambda item: item[1],reverse = True))

next(iter(outdegree_repo_sub.items())),next(iter(indegree_repo_sub.items()))

one_list = repo_influenced_repos['androidtrojan1/android-trojan-starter-']

# # indegree_repo_sub['androidtrojan1/android-trojan-starter-']
# one_list

intersected_keys = set(outdegree_repo_sub.keys()).intersection(set(one_list[i][0] for i in range(len(one_list))))
len(intersected_keys)

"""**Inference from the connected components**

*   There are 35 connected components.
*   Only one of them is a component with 493 nodes, making up 23% of the entire graph,the rest are comparitively very small.


*   After observing the global out_degree_map we realized that the repository with largest outdegree is part of the largest connected component spawning the influence across many nodes, leading to lineage.


*   In fact all the repositories influenced by **'androidtrojan1/android-trojan-starter-'** are part of the subgraph extensing the influence to other repositories.

**Presenting the ancestoral lineage**

We are trying to identify the number of repositories **"androidtrojan1/android-trojan-starter-"** has indirectly influenced.
"""

def print_tree(tree, root):
    level_map = {root: 0}
    levels = {}
    max_level = 0

    # Perform BFS traversal to determine levels of nodes
    queue = [(root, 0)]
    while queue:
        node, level = queue.pop(0)
        max_level = max(max_level, level)
        if level not in levels:
            levels[level] = []
        levels[level].append(node)
        for neighbor in tree.neighbors(node):
            if neighbor not in level_map:
                level_map[neighbor] = level + 1
                queue.append((neighbor, level + 1))
    for lvl in range(max_level + 1):
        if lvl in levels:
            print(f"At level {lvl}: {len(levels[lvl])} {'[root]' if lvl == 0 else ''}node(s): {', '.join(levels[lvl])}")

for repo in one_list:
  val = indegree_only.get(repo[0],0)
  if val!=0:
    print(repo)

import networkx as nx

def dfs(graph, node, parent, tree, visited):
    for neighbor in graph[node]:
        if neighbor != parent:
            if neighbor not in visited:
                tree.add_edge(node, neighbor)
                visited.add(neighbor)
                dfs(graph, neighbor, node, tree, visited)

tree = nx.Graph()
tree.add_node('androidtrojan1/android-trojan-starter-')

visited = set()
dfs(subgraph, 'androidtrojan1/android-trojan-starter-', None, tree, visited)

print_tree(tree,'androidtrojan1/android-trojan-starter-')
# print("Edges of the tree:")
# for edge in tree.edges:
#     print(edge)

"""Knee: 0.1 - 0.3. Hence we take 0.2 as standard for further discussion

How many nodes have no outward edges
"""

indeg_with_repo = sorted(indeg_with_repo, key=lambda item: item[0])
outdeg_with_repo = sorted(outdeg_with_repo, key=lambda item: item[0])

zero_out_count = 0
for tup in outdeg_with_repo:
    if tup[0] == 0:
        zero_out_count += 1

zero_in_count = 0
for tup in indeg_with_repo:
    if tup[0] == 0:
        zero_in_count += 1

(zero_out_count, zero_in_count)

"""19.8% have zero outdegree"""

tpi_map
tpi_map_sorted = sorted(tpi_map.items(), key=lambda item: item[1], reverse=True)

tpi_map_sorted[:10]

influenced_repos_top10 = set()
for repo1, _ in tpi_map_sorted:
  if repo1 in repo_influenced_repos:
    # print(repo_influenced_repos[repo1], type(repo_influenced_repos[repo1]))
    for obj in repo_influenced_repos[repo1]:
      # print(type(obj))
      influenced_repos_top10.add(obj[0])
    # influenced_repos_top10.add(repo_influenced_repos[repo1][0])
print(len(influenced_repos_top10))

repo_influenced_repos, influenced_me_repos = get_influenced_repos(0.2)